{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b485be",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-04T10:40:10.541705Z",
     "iopub.status.busy": "2024-11-04T10:40:10.540903Z",
     "iopub.status.idle": "2024-11-04T10:40:13.630655Z",
     "shell.execute_reply": "2024-11-04T10:40:13.629679Z"
    },
    "papermill": {
     "duration": 3.098486,
     "end_time": "2024-11-04T10:40:13.633019",
     "exception": false,
     "start_time": "2024-11-04T10:40:10.534533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from catboost.utils import get_gpu_device_count\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81c31514",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T10:40:13.643830Z",
     "iopub.status.busy": "2024-11-04T10:40:13.643363Z",
     "iopub.status.idle": "2024-11-04T10:40:13.714032Z",
     "shell.execute_reply": "2024-11-04T10:40:13.713150Z"
    },
    "papermill": {
     "duration": 0.077985,
     "end_time": "2024-11-04T10:40:13.715944",
     "exception": false,
     "start_time": "2024-11-04T10:40:13.637959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPU'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_gpu_available = get_gpu_device_count()\n",
    "device = 'GPU' if is_gpu_available else 'CPU'\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4647d89c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T10:40:13.726785Z",
     "iopub.status.busy": "2024-11-04T10:40:13.726097Z",
     "iopub.status.idle": "2024-11-04T10:40:13.730280Z",
     "shell.execute_reply": "2024-11-04T10:40:13.729469Z"
    },
    "papermill": {
     "duration": 0.011601,
     "end_time": "2024-11-04T10:40:13.732227",
     "exception": false,
     "start_time": "2024-11-04T10:40:13.720626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_PATH = r'/data/Кейс-3. Отток юридических лиц из расчетно-кассового обслуживания/train'\n",
    "TEST_PATH = r'/data/Кейс-3. Отток юридических лиц из расчетно-кассового обслуживания/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90faae2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T10:40:13.742319Z",
     "iopub.status.busy": "2024-11-04T10:40:13.742056Z",
     "iopub.status.idle": "2024-11-04T10:40:45.916488Z",
     "shell.execute_reply": "2024-11-04T10:40:45.915420Z"
    },
    "papermill": {
     "duration": 32.1821,
     "end_time": "2024-11-04T10:40:45.918885",
     "exception": false,
     "start_time": "2024-11-04T10:40:13.736785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    filenames_train = glob.glob(path + '/*.csv')\n",
    "    data_files_train = []\n",
    "    \n",
    "    for filename in filenames_train:\n",
    "        data_files_train.append(pd.read_csv(filename))\n",
    "\n",
    "    return pd.concat(data_files_train, ignore_index=True)\n",
    "\n",
    "train_df = get_data(TRAIN_PATH)\n",
    "test_df = get_data(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15d250c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T10:40:45.929503Z",
     "iopub.status.busy": "2024-11-04T10:40:45.929185Z",
     "iopub.status.idle": "2024-11-04T10:40:45.943878Z",
     "shell.execute_reply": "2024-11-04T10:40:45.942939Z"
    },
    "papermill": {
     "duration": 0.022061,
     "end_time": "2024-11-04T10:40:45.945820",
     "exception": false,
     "start_time": "2024-11-04T10:40:45.923759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ids = test_df['id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe7c722",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T10:40:45.957672Z",
     "iopub.status.busy": "2024-11-04T10:40:45.956965Z",
     "iopub.status.idle": "2024-11-04T10:40:45.962996Z",
     "shell.execute_reply": "2024-11-04T10:40:45.962212Z"
    },
    "papermill": {
     "duration": 0.013275,
     "end_time": "2024-11-04T10:40:45.964897",
     "exception": false,
     "start_time": "2024-11-04T10:40:45.951622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = [\n",
    "       'feature_168', 'feature_87', 'feature_72', 'feature_124', 'feature_141',\n",
    "       'feature_29', 'feature_55', 'feature_142', 'feature_78', 'feature_183',\n",
    "       'feature_84', 'feature_146', 'feature_134', 'feature_26', 'feature_12',\n",
    "       'feature_127', 'feature_59', 'feature_100', 'feature_96', 'feature_112',\n",
    "       'feature_169', 'feature_16', 'feature_76', 'feature_81', 'feature_79',\n",
    "       'feature_22', 'feature_152', 'feature_43', 'feature_20', 'feature_18',\n",
    "       'feature_44', 'id', 'feature_177', 'feature_50', 'feature_6',\n",
    "       'feature_66', 'feature_9', 'feature_46', 'feature_103', 'feature_75',\n",
    "       'feature_8', 'feature_36', 'feature_41', 'feature_184', 'feature_62',\n",
    "       'feature_95', 'feature_133', 'feature_28', 'feature_108', 'feature_128',\n",
    "       'feature_117', 'feature_161', 'feature_157', 'feature_107', 'feature_147'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff26f95b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T10:40:45.975221Z",
     "iopub.status.busy": "2024-11-04T10:40:45.974734Z",
     "iopub.status.idle": "2024-11-04T10:40:46.063234Z",
     "shell.execute_reply": "2024-11-04T10:40:46.062426Z"
    },
    "papermill": {
     "duration": 0.096306,
     "end_time": "2024-11-04T10:40:46.065646",
     "exception": false,
     "start_time": "2024-11-04T10:40:45.969340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = train_df[f + ['target']]\n",
    "test_df = test_df[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fadb9bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T10:40:46.076802Z",
     "iopub.status.busy": "2024-11-04T10:40:46.076173Z",
     "iopub.status.idle": "2024-11-04T10:40:46.111683Z",
     "shell.execute_reply": "2024-11-04T10:40:46.110651Z"
    },
    "papermill": {
     "duration": 0.043507,
     "end_time": "2024-11-04T10:40:46.114020",
     "exception": false,
     "start_time": "2024-11-04T10:40:46.070513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "feature_operations = {\n",
    "    'atan_feature_112': ('feature_112', np.arctan),\n",
    "    'log_feature_26': ('feature_26', np.log1p),\n",
    "    'sin_feature_26': ('feature_26', np.sin),\n",
    "    'tan_feature_26': ('feature_26', np.tan)\n",
    "}\n",
    "\n",
    "for new_feature, (base_feature, operation) in feature_operations.items():\n",
    "    train_df[new_feature] = operation(train_df[base_feature])\n",
    "    test_df[new_feature] = operation(test_df[base_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2edeed0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T10:40:46.125273Z",
     "iopub.status.busy": "2024-11-04T10:40:46.124988Z",
     "iopub.status.idle": "2024-11-04T10:44:27.612623Z",
     "shell.execute_reply": "2024-11-04T10:44:27.611613Z"
    },
    "papermill": {
     "duration": 221.496026,
     "end_time": "2024-11-04T10:44:27.615071",
     "exception": false,
     "start_time": "2024-11-04T10:40:46.119045",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/Markdown-3.6-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/PySocks-1.7.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/absl_py-2.1.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/accelerate-0.21.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/aiohttp_cors-0.7.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/aiosignal-1.3.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/annotated_types-0.7.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/async_timeout-4.0.3-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/attrs-23.2.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/autogluon-1.1.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/autogluon.common-1.1.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/autogluon.core-1.1.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/autogluon.features-1.1.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/autogluon.multimodal-1.1.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/autogluon.tabular-1.1.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/autogluon.timeseries-1.1.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/beautifulsoup4-4.12.3-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/blis-0.7.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/boto3-1.34.144-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/botocore-1.34.144-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/cachetools-5.4.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/catalogue-2.0.10-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/certifi-2024.7.4-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/click-8.1.7-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/cloudpathlib-0.18.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/cloudpickle-3.0.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/colorama-0.4.6-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/coloredlogs-15.0.1-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/colorful-0.5.6-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/confection-0.1.5-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/cycler-0.12.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/cymem-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/datasets-2.20.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/defusedxml-0.7.1-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/dill-0.3.8-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/distlib-0.3.8-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/evaluate-0.4.2-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/fastai-2.7.15-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/fastcore-1.5.54-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/fastdownload-0.0.7-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/fastprogress-1.0.3-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/filelock-3.15.4-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/flatbuffers-24.3.25-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/fonttools-4.53.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/fsspec-2024.5.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/future-1.0.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/gdown-5.2.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/gluonts-0.15.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/google_api_core-2.19.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/google_auth-2.32.0-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/googleapis_common_protos-1.63.2-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/graphviz-0.20.3-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/grpcio-1.64.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/huggingface_hub-0.23.4-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/humanfriendly-10.0-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/hyperopt-0.2.7-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/idna-3.7-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/imageio-2.34.2-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/jinja2-3.1.4-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/jmespath-1.0.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/joblib-1.4.2-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/jsonschema-4.21.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/jsonschema_specifications-2023.12.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/langcodes-3.4.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/language_data-1.2.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/lazy_loader-0.4-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/lightgbm-4.3.0-py3-none-manylinux_2_28_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/lightning-2.3.3-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/lightning_utilities-0.11.5-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/llvmlite-0.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/marisa_trie-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/markdown_it_py-3.0.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/matplotlib-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/mdurl-0.1.2-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/mlforecast-0.10.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/model_index-0.1.11-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/mpmath-1.3.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/msgpack-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/multiprocess-0.70.16-py310-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/murmurhash-1.0.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/networkx-3.3-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/nlpaug-1.1.11-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/nltk-3.8.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/nptyping-2.4.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/numba-0.60.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/omegaconf-2.2.3-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/onnx-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/opencensus-0.11.4-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/opencensus_context-0.1.3-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/opendatalab-0.0.10-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/openmim-0.3.9-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/openxlab-0.0.11-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/optimum-1.18.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/ordered_set-4.1.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/packaging-24.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/patsy-0.5.6-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/pdf2image-1.17.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/pip-24.1.2-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/platformdirs-4.2.2-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/plotly-5.22.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/preshed-3.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/prometheus_client-0.20.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/proto_plus-1.24.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/py4j-0.10.9.7-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/pyarrow_hotfix-0.6-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/pyasn1-0.6.0-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/pyasn1_modules-0.4.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/pydantic-2.8.2-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/pygments-2.18.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/pyparsing-3.1.2-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/pytesseract-0.3.10-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/python_dateutil-2.9.0.post0-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/pytorch_lightning-2.3.3-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/pytorch_metric_learning-2.3.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/pytz-2024.1-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/pywavelets-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/referencing-0.35.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/requests-2.32.3-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/rich-13.7.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/rpds_py-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/rsa-4.9-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/s3transfer-0.10.2-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/scikit_image-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/setuptools-70.3.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/shellingham-1.5.4-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/six-1.16.0-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/smart_open-7.0.4-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/soupsieve-2.5-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/spacy-3.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/spacy_legacy-3.0.12-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/spacy_loggers-1.0.5-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/srsly-2.4.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/statsforecast-1.4.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/statsmodels-0.14.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/sympy-1.13.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/tabulate-0.9.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/tenacity-8.5.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/tensorboard-2.17.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/tensorboardX-2.6.2.2-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/text_unidecode-1.3-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/thinc-8.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/threadpoolctl-3.5.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/tifffile-2024.7.2-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/timm-0.9.16-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/toolz-0.12.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/torchmetrics-1.2.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/tqdm-4.66.4-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/transformers-4.39.3-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/typer-0.12.3-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/typing_extensions-4.12.2-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/tzdata-2024.1-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/urllib3-2.2.2-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/utilsforecast-0.0.10-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/virtualenv-20.26.3-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/wasabi-1.1.3-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/weasel-0.4.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/werkzeug-3.0.3-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/window_ops-0.0.15-py3-none-any.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/autogluon-zaebal/autogluon-setupchik/yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Markdown is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "MarkupSafe is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "PySocks is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "aiohttp is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "aiosignal is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "annotated-types is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "async-timeout is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "attrs is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "beautifulsoup4 is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "catalogue is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "charset-normalizer is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "click is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "cloudpickle is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "colorama is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "colorful is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "contourpy is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "cycler is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "cymem is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "defusedxml is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "dill is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "distlib is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "fastdownload is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "fastprogress is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "flatbuffers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "frozenlist is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "future is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "graphviz is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "hyperopt is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "idna is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "jinja2 is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "jmespath is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "joblib is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "jsonschema-specifications is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "kiwisolver is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "language-data is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "lazy-loader is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "llvmlite is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "markdown-it-py is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "mdurl is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "mpmath is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "msgpack is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "multidict is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "multiprocess is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "murmurhash is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "networkx is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "numba is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "numpy is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "opencensus is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "opencensus-context is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "pandas is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "patsy is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "pdf2image is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "plotly is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "preshed is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "prometheus-client is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "py4j is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "py-spy is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "pyarrow is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "pyarrow-hotfix is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "pyasn1 is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "pyasn1-modules is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "pycryptodome is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "pygments is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "pyparsing is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "python-dateutil is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "pytz is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "pywavelets is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "referencing is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "regex is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "requests is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "rich is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "rsa is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "sentencepiece is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "shellingham is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "six is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "smart-open is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "soupsieve is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "spacy-legacy is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "spacy-loggers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "srsly is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "statsmodels is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "tabulate is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "tensorboardX is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "tensorboard-data-server is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "text-unidecode is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "thinc is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "threadpoolctl is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "toolz is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "tqdm is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "typer is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "typing-extensions is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "tzdata is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "weasel is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "wrapt is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "xgboost is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "xxhash is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "yarl is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "Installing collected packages: triton, onnxruntime, model-index, catboost, blis, aiohttp-cors, window-ops, werkzeug, wasabi, virtualenv, utilsforecast, urllib3, transformers, torchvision, torchmetrics, torch, tokenizers, timm, tifffile, tensorboard, tenacity, sympy, statsforecast, spacy, setuptools, scipy, scikit-learn, scikit-image, safetensors, s3transfer, rpds-py, ray, PyYAML, pytorch-metric-learning, pytorch-lightning, pytesseract, pydantic-core, pydantic, psutil, protobuf, proto-plus, platformdirs, pip, pillow, packaging, orjson, ordered-set, optimum, openxlab, openmim, opendatalab, onnx, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-cusolver-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nptyping, nltk, nlpaug, mlforecast, matplotlib, marisa-trie, lightning-utilities, lightning, lightgbm, langcodes, jsonschema, imageio, humanfriendly, huggingface-hub, grpcio, googleapis-common-protos, google-auth, google-api-core, gluonts, gdown, fsspec, fonttools, filelock, fastcore, fastai, evaluate, datasets, confection, coloredlogs, cloudpathlib, certifi, cachetools, botocore, boto3, autogluon.timeseries, autogluon.tabular, autogluon.multimodal, autogluon.features, autogluon.core, autogluon.common, autogluon, accelerate, absl-py\r\n",
      "  Attempting uninstall: catboost\r\n",
      "    Found existing installation: catboost 1.2.7\r\n",
      "    Uninstalling catboost-1.2.7:\r\n",
      "      Successfully uninstalled catboost-1.2.7\r\n",
      "  Attempting uninstall: blis\r\n",
      "    Found existing installation: blis 0.7.10\r\n",
      "    Uninstalling blis-0.7.10:\r\n",
      "      Successfully uninstalled blis-0.7.10\r\n",
      "  Attempting uninstall: werkzeug\r\n",
      "    Found existing installation: Werkzeug 3.0.4\r\n",
      "    Uninstalling Werkzeug-3.0.4:\r\n",
      "      Successfully uninstalled Werkzeug-3.0.4\r\n",
      "  Attempting uninstall: wasabi\r\n",
      "    Found existing installation: wasabi 1.1.2\r\n",
      "    Uninstalling wasabi-1.1.2:\r\n",
      "      Successfully uninstalled wasabi-1.1.2\r\n",
      "  Attempting uninstall: virtualenv\r\n",
      "    Found existing installation: virtualenv 20.21.0\r\n",
      "    Uninstalling virtualenv-20.21.0:\r\n",
      "      Successfully uninstalled virtualenv-20.21.0\r\n",
      "  Attempting uninstall: urllib3\r\n",
      "    Found existing installation: urllib3 1.26.18\r\n",
      "    Uninstalling urllib3-1.26.18:\r\n",
      "      Successfully uninstalled urllib3-1.26.18\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.45.1\r\n",
      "    Uninstalling transformers-4.45.1:\r\n",
      "      Successfully uninstalled transformers-4.45.1\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.19.0\r\n",
      "    Uninstalling torchvision-0.19.0:\r\n",
      "      Successfully uninstalled torchvision-0.19.0\r\n",
      "  Attempting uninstall: torchmetrics\r\n",
      "    Found existing installation: torchmetrics 1.4.2\r\n",
      "    Uninstalling torchmetrics-1.4.2:\r\n",
      "      Successfully uninstalled torchmetrics-1.4.2\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.4.0\r\n",
      "    Uninstalling torch-2.4.0:\r\n",
      "      Successfully uninstalled torch-2.4.0\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.20.0\r\n",
      "    Uninstalling tokenizers-0.20.0:\r\n",
      "      Successfully uninstalled tokenizers-0.20.0\r\n",
      "  Attempting uninstall: timm\r\n",
      "    Found existing installation: timm 1.0.9\r\n",
      "    Uninstalling timm-1.0.9:\r\n",
      "      Successfully uninstalled timm-1.0.9\r\n",
      "  Attempting uninstall: tifffile\r\n",
      "    Found existing installation: tifffile 2024.5.22\r\n",
      "    Uninstalling tifffile-2024.5.22:\r\n",
      "      Successfully uninstalled tifffile-2024.5.22\r\n",
      "  Attempting uninstall: tensorboard\r\n",
      "    Found existing installation: tensorboard 2.16.2\r\n",
      "    Uninstalling tensorboard-2.16.2:\r\n",
      "      Successfully uninstalled tensorboard-2.16.2\r\n",
      "  Attempting uninstall: tenacity\r\n",
      "    Found existing installation: tenacity 8.3.0\r\n",
      "    Uninstalling tenacity-8.3.0:\r\n",
      "      Successfully uninstalled tenacity-8.3.0\r\n",
      "  Attempting uninstall: sympy\r\n",
      "    Found existing installation: sympy 1.13.3\r\n",
      "    Uninstalling sympy-1.13.3:\r\n",
      "      Successfully uninstalled sympy-1.13.3\r\n",
      "  Attempting uninstall: spacy\r\n",
      "    Found existing installation: spacy 3.7.6\r\n",
      "    Uninstalling spacy-3.7.6:\r\n",
      "      Successfully uninstalled spacy-3.7.6\r\n",
      "  Attempting uninstall: setuptools\r\n",
      "    Found existing installation: setuptools 70.0.0\r\n",
      "    Uninstalling setuptools-70.0.0:\r\n",
      "      Successfully uninstalled setuptools-70.0.0\r\n",
      "  Attempting uninstall: scipy\r\n",
      "    Found existing installation: scipy 1.14.1\r\n",
      "    Uninstalling scipy-1.14.1:\r\n",
      "      Successfully uninstalled scipy-1.14.1\r\n",
      "  Attempting uninstall: scikit-learn\r\n",
      "    Found existing installation: scikit-learn 1.2.2\r\n",
      "    Uninstalling scikit-learn-1.2.2:\r\n",
      "      Successfully uninstalled scikit-learn-1.2.2\r\n",
      "  Attempting uninstall: scikit-image\r\n",
      "    Found existing installation: scikit-image 0.23.2\r\n",
      "    Uninstalling scikit-image-0.23.2:\r\n",
      "      Successfully uninstalled scikit-image-0.23.2\r\n",
      "  Attempting uninstall: safetensors\r\n",
      "    Found existing installation: safetensors 0.4.5\r\n",
      "    Uninstalling safetensors-0.4.5:\r\n",
      "      Successfully uninstalled safetensors-0.4.5\r\n",
      "  Attempting uninstall: s3transfer\r\n",
      "    Found existing installation: s3transfer 0.6.2\r\n",
      "    Uninstalling s3transfer-0.6.2:\r\n",
      "      Successfully uninstalled s3transfer-0.6.2\r\n",
      "  Attempting uninstall: rpds-py\r\n",
      "    Found existing installation: rpds-py 0.18.1\r\n",
      "    Uninstalling rpds-py-0.18.1:\r\n",
      "      Successfully uninstalled rpds-py-0.18.1\r\n",
      "  Attempting uninstall: ray\r\n",
      "    Found existing installation: ray 2.24.0\r\n",
      "    Uninstalling ray-2.24.0:\r\n",
      "      Successfully uninstalled ray-2.24.0\r\n",
      "  Attempting uninstall: PyYAML\r\n",
      "    Found existing installation: PyYAML 6.0.2\r\n",
      "    Uninstalling PyYAML-6.0.2:\r\n",
      "      Successfully uninstalled PyYAML-6.0.2\r\n",
      "  Attempting uninstall: pytorch-lightning\r\n",
      "    Found existing installation: pytorch-lightning 2.4.0\r\n",
      "    Uninstalling pytorch-lightning-2.4.0:\r\n",
      "      Successfully uninstalled pytorch-lightning-2.4.0\r\n",
      "  Attempting uninstall: pytesseract\r\n",
      "    Found existing installation: pytesseract 0.3.13\r\n",
      "    Uninstalling pytesseract-0.3.13:\r\n",
      "      Successfully uninstalled pytesseract-0.3.13\r\n",
      "  Attempting uninstall: pydantic-core\r\n",
      "    Found existing installation: pydantic_core 2.23.4\r\n",
      "    Uninstalling pydantic_core-2.23.4:\r\n",
      "      Successfully uninstalled pydantic_core-2.23.4\r\n",
      "  Attempting uninstall: pydantic\r\n",
      "    Found existing installation: pydantic 2.9.2\r\n",
      "    Uninstalling pydantic-2.9.2:\r\n",
      "      Successfully uninstalled pydantic-2.9.2\r\n",
      "  Attempting uninstall: psutil\r\n",
      "    Found existing installation: psutil 5.9.3\r\n",
      "    Uninstalling psutil-5.9.3:\r\n",
      "      Successfully uninstalled psutil-5.9.3\r\n",
      "  Attempting uninstall: protobuf\r\n",
      "    Found existing installation: protobuf 3.20.3\r\n",
      "    Uninstalling protobuf-3.20.3:\r\n",
      "      Successfully uninstalled protobuf-3.20.3\r\n",
      "  Attempting uninstall: proto-plus\r\n",
      "    Found existing installation: proto-plus 1.23.0\r\n",
      "    Uninstalling proto-plus-1.23.0:\r\n",
      "      Successfully uninstalled proto-plus-1.23.0\r\n",
      "  Attempting uninstall: platformdirs\r\n",
      "    Found existing installation: platformdirs 3.11.0\r\n",
      "    Uninstalling platformdirs-3.11.0:\r\n",
      "      Successfully uninstalled platformdirs-3.11.0\r\n",
      "  Attempting uninstall: pip\r\n",
      "    Found existing installation: pip 24.0\r\n",
      "    Uninstalling pip-24.0:\r\n",
      "      Successfully uninstalled pip-24.0\r\n",
      "  Attempting uninstall: pillow\r\n",
      "    Found existing installation: pillow 10.3.0\r\n",
      "    Uninstalling pillow-10.3.0:\r\n",
      "      Successfully uninstalled pillow-10.3.0\r\n",
      "  Attempting uninstall: packaging\r\n",
      "    Found existing installation: packaging 21.3\r\n",
      "    Uninstalling packaging-21.3:\r\n",
      "      Successfully uninstalled packaging-21.3\r\n",
      "  Attempting uninstall: orjson\r\n",
      "    Found existing installation: orjson 3.10.4\r\n",
      "    Uninstalling orjson-3.10.4:\r\n",
      "      Successfully uninstalled orjson-3.10.4\r\n",
      "  Attempting uninstall: onnx\r\n",
      "    Found existing installation: onnx 1.17.0\r\n",
      "    Uninstalling onnx-1.17.0:\r\n",
      "      Successfully uninstalled onnx-1.17.0\r\n",
      "  Attempting uninstall: nltk\r\n",
      "    Found existing installation: nltk 3.2.4\r\n",
      "    Uninstalling nltk-3.2.4:\r\n",
      "      Successfully uninstalled nltk-3.2.4\r\n",
      "  Attempting uninstall: matplotlib\r\n",
      "    Found existing installation: matplotlib 3.7.5\r\n",
      "    Uninstalling matplotlib-3.7.5:\r\n",
      "      Successfully uninstalled matplotlib-3.7.5\r\n",
      "  Attempting uninstall: marisa-trie\r\n",
      "    Found existing installation: marisa-trie 1.1.0\r\n",
      "    Uninstalling marisa-trie-1.1.0:\r\n",
      "      Successfully uninstalled marisa-trie-1.1.0\r\n",
      "  Attempting uninstall: lightning-utilities\r\n",
      "    Found existing installation: lightning-utilities 0.11.7\r\n",
      "    Uninstalling lightning-utilities-0.11.7:\r\n",
      "      Successfully uninstalled lightning-utilities-0.11.7\r\n",
      "  Attempting uninstall: lightgbm\r\n",
      "    Found existing installation: lightgbm 4.2.0\r\n",
      "    Uninstalling lightgbm-4.2.0:\r\n",
      "      Successfully uninstalled lightgbm-4.2.0\r\n",
      "  Attempting uninstall: langcodes\r\n",
      "    Found existing installation: langcodes 3.4.1\r\n",
      "    Uninstalling langcodes-3.4.1:\r\n",
      "      Successfully uninstalled langcodes-3.4.1\r\n",
      "  Attempting uninstall: jsonschema\r\n",
      "    Found existing installation: jsonschema 4.22.0\r\n",
      "    Uninstalling jsonschema-4.22.0:\r\n",
      "      Successfully uninstalled jsonschema-4.22.0\r\n",
      "  Attempting uninstall: imageio\r\n",
      "    Found existing installation: imageio 2.34.1\r\n",
      "    Uninstalling imageio-2.34.1:\r\n",
      "      Successfully uninstalled imageio-2.34.1\r\n",
      "  Attempting uninstall: huggingface-hub\r\n",
      "    Found existing installation: huggingface-hub 0.25.1\r\n",
      "    Uninstalling huggingface-hub-0.25.1:\r\n",
      "      Successfully uninstalled huggingface-hub-0.25.1\r\n",
      "  Attempting uninstall: grpcio\r\n",
      "    Found existing installation: grpcio 1.62.2\r\n",
      "    Uninstalling grpcio-1.62.2:\r\n",
      "      Successfully uninstalled grpcio-1.62.2\r\n",
      "  Attempting uninstall: googleapis-common-protos\r\n",
      "    Found existing installation: googleapis-common-protos 1.63.1\r\n",
      "    Uninstalling googleapis-common-protos-1.63.1:\r\n",
      "      Successfully uninstalled googleapis-common-protos-1.63.1\r\n",
      "  Attempting uninstall: google-auth\r\n",
      "    Found existing installation: google-auth 2.30.0\r\n",
      "    Uninstalling google-auth-2.30.0:\r\n",
      "      Successfully uninstalled google-auth-2.30.0\r\n",
      "  Attempting uninstall: google-api-core\r\n",
      "    Found existing installation: google-api-core 2.11.1\r\n",
      "    Uninstalling google-api-core-2.11.1:\r\n",
      "      Successfully uninstalled google-api-core-2.11.1\r\n",
      "  Attempting uninstall: fsspec\r\n",
      "    Found existing installation: fsspec 2024.6.1\r\n",
      "    Uninstalling fsspec-2024.6.1:\r\n",
      "      Successfully uninstalled fsspec-2024.6.1\r\n",
      "  Attempting uninstall: fonttools\r\n",
      "    Found existing installation: fonttools 4.53.0\r\n",
      "    Uninstalling fonttools-4.53.0:\r\n",
      "      Successfully uninstalled fonttools-4.53.0\r\n",
      "  Attempting uninstall: filelock\r\n",
      "    Found existing installation: filelock 3.15.1\r\n",
      "    Uninstalling filelock-3.15.1:\r\n",
      "      Successfully uninstalled filelock-3.15.1\r\n",
      "  Attempting uninstall: fastcore\r\n",
      "    Found existing installation: fastcore 1.7.10\r\n",
      "    Uninstalling fastcore-1.7.10:\r\n",
      "      Successfully uninstalled fastcore-1.7.10\r\n",
      "  Attempting uninstall: fastai\r\n",
      "    Found existing installation: fastai 2.7.17\r\n",
      "    Uninstalling fastai-2.7.17:\r\n",
      "      Successfully uninstalled fastai-2.7.17\r\n",
      "  Attempting uninstall: datasets\r\n",
      "    Found existing installation: datasets 3.0.1\r\n",
      "    Uninstalling datasets-3.0.1:\r\n",
      "      Successfully uninstalled datasets-3.0.1\r\n",
      "  Attempting uninstall: confection\r\n",
      "    Found existing installation: confection 0.1.4\r\n",
      "    Uninstalling confection-0.1.4:\r\n",
      "      Successfully uninstalled confection-0.1.4\r\n",
      "  Attempting uninstall: cloudpathlib\r\n",
      "    Found existing installation: cloudpathlib 0.19.0\r\n",
      "    Uninstalling cloudpathlib-0.19.0:\r\n",
      "      Successfully uninstalled cloudpathlib-0.19.0\r\n",
      "  Attempting uninstall: certifi\r\n",
      "    Found existing installation: certifi 2024.8.30\r\n",
      "    Uninstalling certifi-2024.8.30:\r\n",
      "      Successfully uninstalled certifi-2024.8.30\r\n",
      "  Attempting uninstall: cachetools\r\n",
      "    Found existing installation: cachetools 4.2.4\r\n",
      "    Uninstalling cachetools-4.2.4:\r\n",
      "      Successfully uninstalled cachetools-4.2.4\r\n",
      "  Attempting uninstall: botocore\r\n",
      "    Found existing installation: botocore 1.35.23\r\n",
      "    Uninstalling botocore-1.35.23:\r\n",
      "      Successfully uninstalled botocore-1.35.23\r\n",
      "  Attempting uninstall: boto3\r\n",
      "    Found existing installation: boto3 1.26.100\r\n",
      "    Uninstalling boto3-1.26.100:\r\n",
      "      Successfully uninstalled boto3-1.26.100\r\n",
      "  Attempting uninstall: accelerate\r\n",
      "    Found existing installation: accelerate 0.34.2\r\n",
      "    Uninstalling accelerate-0.34.2:\r\n",
      "      Successfully uninstalled accelerate-0.34.2\r\n",
      "  Attempting uninstall: absl-py\r\n",
      "    Found existing installation: absl-py 1.4.0\r\n",
      "    Uninstalling absl-py-1.4.0:\r\n",
      "      Successfully uninstalled absl-py-1.4.0\r\n",
      "Successfully installed PyYAML-6.0.1 absl-py-2.1.0 accelerate-0.21.0 aiohttp-cors-0.7.0 autogluon-1.1.1 autogluon.common-1.1.1 autogluon.core-1.1.1 autogluon.features-1.1.1 autogluon.multimodal-1.1.1 autogluon.tabular-1.1.1 autogluon.timeseries-1.1.1 blis-0.7.11 boto3-1.34.144 botocore-1.34.144 cachetools-5.3.3 catboost-1.2.5 certifi-2024.7.4 cloudpathlib-0.18.1 coloredlogs-15.0.1 confection-0.1.5 datasets-2.20.0 evaluate-0.4.2 fastai-2.7.15 fastcore-1.5.54 filelock-3.15.4 fonttools-4.53.1 fsspec-2024.5.0 gdown-5.2.0 gluonts-0.15.1 google-api-core-2.19.1 google-auth-2.32.0 googleapis-common-protos-1.63.2 grpcio-1.64.1 huggingface-hub-0.23.4 humanfriendly-10.0 imageio-2.34.2 jsonschema-4.21.1 langcodes-3.4.0 lightgbm-4.3.0 lightning-2.3.3 lightning-utilities-0.11.5 marisa-trie-1.2.0 matplotlib-3.9.1 mlforecast-0.10.0 model-index-0.1.11 nlpaug-1.1.11 nltk-3.8.1 nptyping-2.4.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 omegaconf-2.2.3 onnx-1.16.1 onnxruntime-1.18.1 opendatalab-0.0.10 openmim-0.3.9 openxlab-0.0.11 optimum-1.18.1 ordered-set-4.1.0 orjson-3.10.6 packaging-24.1 pillow-10.4.0 pip-24.1.2 platformdirs-4.2.2 proto-plus-1.24.0 protobuf-4.25.3 psutil-5.9.8 pydantic-2.8.2 pydantic-core-2.20.1 pytesseract-0.3.10 pytorch-lightning-2.3.3 pytorch-metric-learning-2.3.0 ray-2.10.0 rpds-py-0.19.0 s3transfer-0.10.2 safetensors-0.4.3 scikit-image-0.20.0 scikit-learn-1.4.0 scipy-1.12.0 setuptools-70.3.0 spacy-3.7.5 statsforecast-1.4.0 sympy-1.13.0 tenacity-8.5.0 tensorboard-2.17.0 tifffile-2024.7.2 timm-0.9.16 tokenizers-0.15.2 torch-2.3.1 torchmetrics-1.2.1 torchvision-0.18.1 transformers-4.39.3 triton-2.3.1 urllib3-2.2.1 utilsforecast-0.0.10 virtualenv-20.26.3 wasabi-1.1.3 werkzeug-3.0.3 window-ops-0.0.15\r\n"
     ]
    }
   ],
   "source": [
    "!pip install \"autogluon==1.1.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3eb31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T10:44:27.668628Z",
     "iopub.status.busy": "2024-11-04T10:44:27.668223Z",
     "iopub.status.idle": "2024-11-04T10:44:35.756496Z",
     "shell.execute_reply": "2024-11-04T10:44:35.755376Z"
    },
    "papermill": {
     "duration": 8.118109,
     "end_time": "2024-11-04T10:44:35.758926",
     "exception": false,
     "start_time": "2024-11-04T10:44:27.640817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/sklearn-zaebal/sklearn-setupchik/scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2) (1.12.0)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2) (3.5.0)\r\n",
      "Installing collected packages: scikit-learn\r\n",
      "  Attempting uninstall: scikit-learn\r\n",
      "    Found existing installation: scikit-learn 1.4.0\r\n",
      "    Uninstalling scikit-learn-1.4.0:\r\n",
      "      Successfully uninstalled scikit-learn-1.4.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "autogluon-multimodal 1.1.1 requires nvidia-ml-py3==7.352.0, which is not installed.\r\n",
      "autogluon-multimodal 1.1.1 requires seqeval<1.3.0,>=1.2.2, which is not installed.\r\n",
      "autogluon-core 1.1.1 requires scikit-learn<1.4.1,>=1.3.0, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "autogluon-features 1.1.1 requires scikit-learn<1.4.1,>=1.3.0, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "autogluon-multimodal 1.1.1 requires scikit-learn<1.4.1,>=1.3.0, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "autogluon-tabular 1.1.1 requires scikit-learn<1.4.1,>=1.3.0, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "bigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\r\n",
      "bigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "bigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.2 which is incompatible.\r\n",
      "cesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "dataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.8.2 which is incompatible.\r\n",
      "tsfresh 0.20.3 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.12.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed scikit-learn-1.2.2\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install \"scikit_learn==1.2.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1c3c52f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T10:44:35.812659Z",
     "iopub.status.busy": "2024-11-04T10:44:35.811852Z",
     "iopub.status.idle": "2024-11-04T10:44:36.740849Z",
     "shell.execute_reply": "2024-11-04T10:44:36.739865Z"
    },
    "papermill": {
     "duration": 0.958163,
     "end_time": "2024-11-04T10:44:36.743188",
     "exception": false,
     "start_time": "2024-11-04T10:44:35.785025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1d8339d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T10:44:36.796575Z",
     "iopub.status.busy": "2024-11-04T10:44:36.795793Z",
     "iopub.status.idle": "2024-11-04T10:44:36.800570Z",
     "shell.execute_reply": "2024-11-04T10:44:36.799715Z"
    },
    "papermill": {
     "duration": 0.033178,
     "end_time": "2024-11-04T10:44:36.802381",
     "exception": false,
     "start_time": "2024-11-04T10:44:36.769203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_tab = TabularDataset(train_df)\n",
    "test_tab = TabularDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec69b44d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T10:44:36.856144Z",
     "iopub.status.busy": "2024-11-04T10:44:36.855341Z",
     "iopub.status.idle": "2024-11-04T11:34:21.797124Z",
     "shell.execute_reply": "2024-11-04T11:34:21.796312Z"
    },
    "papermill": {
     "duration": 2984.970014,
     "end_time": "2024-11-04T11:34:21.799129",
     "exception": false,
     "start_time": "2024-11-04T10:44:36.829115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20241104_104436\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.14\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       29.83 GB / 31.36 GB (95.1%)\n",
      "Disk Space Avail:   19.50 GB / 19.52 GB (99.9%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 9900s of the 39600s of remaining time (25%).\n",
      "2024-11-04 10:44:37,172\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2024-11-04 10:44:39,735\tINFO worker.py:1743 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"AutogluonModels/ag-20241104_104436/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Beginning AutoGluon training ... Time limit = 9896s\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m AutoGluon will save models to \"AutogluonModels/ag-20241104_104436/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Train Data Rows:    367283\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Train Data Columns: 59\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Label Column:       target\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Problem Type:       binary\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tAvailable Memory:                    29683.98 MB\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tTrain Data (Original)  Memory Usage: 165.33 MB (0.6% of available memory)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\t('float', []) : 58 | ['feature_168', 'feature_87', 'feature_72', 'feature_124', 'feature_141', ...]\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\t('int', [])   :  1 | ['id']\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\t('float', []) : 58 | ['feature_168', 'feature_87', 'feature_72', 'feature_124', 'feature_141', ...]\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\t('int', [])   :  1 | ['id']\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t3.6s = Fit runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t59 features in original data used to generate 59 features in processed data.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tTrain Data (Processed) Memory Usage: 165.33 MB (0.6% of available memory)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Data preprocessing and feature engineering runtime = 3.98s ...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t'CAT': {'auto_class_weights': 'Balanced'},\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t'XGB': {},\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting 5 L1 models ...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 6592.86s of the 9891.76s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=3.76%)\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=526)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=561)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=596)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=631)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=666)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=701)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=736)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.8106\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t221.01s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t10.65s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 6363.89s of the 9662.79s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=3.79%)\n",
      "\u001b[36m(_ray_fit pid=849)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=884)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=919)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=954)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=989)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1024)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1059)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1094)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.8094\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t146.06s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t4.39s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 6212.76s of the 9511.66s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=3.83%)\n",
      "\u001b[36m(_ray_fit pid=1207)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1207)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=1251)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1251)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=1295)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1295)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=1339)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1339)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=1383)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1383)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=1427)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1427)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=1471)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1471)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=1515)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1515)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.8074\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t112.65s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.24s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 6095.64s of the 9394.54s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=4.98%)\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:16] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:16] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:18] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:18] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:27] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:27] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:29] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:29] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=1699)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:38] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=1699)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1699)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1699)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1699)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1699)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:40] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1699)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1699)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:40] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=1699)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=1699)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=1699)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=1699)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=1730)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:49] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=1730)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1730)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1730)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:49] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1730)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1730)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:51] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1730)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1730)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:51] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=1730)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=1730)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=1730)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=1730)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:00] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:00] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:01] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:01] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=1792)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:11] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=1792)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1792)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1792)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:11] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1792)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1792)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:13] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1792)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1792)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:13] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=1792)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=1792)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=1792)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=1792)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=1823)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:22] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=1823)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1823)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1823)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:22] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1823)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1823)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:24] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1823)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1823)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:24] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=1823)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=1823)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=1823)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=1823)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:33] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:33] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:34] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:34] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.8108\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t85.29s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t1.96s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 6005.63s of the 9304.53s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=3.90%)\n",
      "\u001b[36m(_ray_fit pid=1963)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1998)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2033)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2068)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2103)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2138)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2173)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2208)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.8104\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t226.95s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t6.13s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 659.29s of the 9072.1s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.353, 'XGBoost_BAG_L1': 0.353, 'LightGBMLarge_BAG_L1': 0.294}\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.8126\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t9.03s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.07s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting 5 L2 models ...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 9062.96s of the 9062.86s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=3.97%)\n",
      "\u001b[36m(_ray_fit pid=2327)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2362)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2397)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2432)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2467)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2502)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2537)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2572)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.8128\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t114.88s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t2.36s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 8943.67s of the 8943.58s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=3.97%)\n",
      "\u001b[36m(_ray_fit pid=2691)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2726)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2761)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2796)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2831)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2866)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2901)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2936)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.8124\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t107.71s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t1.4s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 8831.71s of the 8831.62s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=4.01%)\n",
      "\u001b[36m(_ray_fit pid=3055)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3055)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=3099)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3099)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=3143)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3143)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=3187)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3187)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=3231)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3231)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=3275)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3275)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=3319)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3319)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=3363)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3363)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.8127\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t100.02s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.25s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: XGBoost_BAG_L2 ... Training model for up to 8727.49s of the 8727.4s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=5.22%)\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:24] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:24] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:25] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:25] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=3522)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:34] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=3522)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3522)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:34] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3522)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3522)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3522)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:35] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3522)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3522)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:35] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=3522)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=3522)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=3522)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=3522)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=3553)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:45] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=3553)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3553)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3553)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:45] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3553)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3553)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:46] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3553)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3553)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:46] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=3553)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=3553)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=3553)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=3553)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=3584)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:56] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=3584)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3584)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3584)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:56] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3584)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3584)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:57] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3584)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3584)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:57] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=3584)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=3584)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=3584)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=3584)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=3615)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:07] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=3615)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3615)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3615)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:07] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3615)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3615)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:08] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3615)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3615)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:08] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=3615)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=3615)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=3615)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=3615)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=3646)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:18] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=3646)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3646)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3646)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:18] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3646)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3646)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:19] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3646)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3646)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:19] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=3646)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=3646)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=3646)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=3646)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=3677)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:28] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=3677)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3677)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3677)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3677)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3677)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:30] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3677)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3677)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:30] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=3677)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=3677)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=3677)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=3677)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:39] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:39] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:40] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:40] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.8125\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t84.32s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t2.24s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 8638.61s of the 8638.52s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=4.10%)\n",
      "\u001b[36m(_ray_fit pid=3823)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3858)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3893)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3928)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3963)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3998)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=4033)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=4068)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.8115\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t166.3s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t2.66s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 906.29s of the 8467.66s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.35, 'XGBoost_BAG_L2': 0.3, 'XGBoost_BAG_L1': 0.1, 'LightGBMLarge_BAG_L1': 0.1, 'LightGBMXT_BAG_L1': 0.05, 'LightGBM_BAG_L2': 0.05, 'LightGBMLarge_BAG_L2': 0.05}\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.8133\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t13.98s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.07s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m AutoGluon training complete, total runtime = 1442.46s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1432.7 rows/s (45911 batch size)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241104_104436/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                   model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0        CatBoost_BAG_L2       0.815692   0.812735     roc_auc       12.462878      23.630606   891.990016                 0.295594                0.254583         100.021352            2       True          9\n",
      "1        LightGBM_BAG_L2       0.815579   0.812390     roc_auc       12.926130      24.778124   899.680461                 0.758845                1.402102         107.711797            2       True          8\n",
      "2      LightGBMXT_BAG_L2       0.815572   0.812794     roc_auc       13.227981      25.736464   906.847595                 1.060697                2.360441         114.878931            2       True          7\n",
      "3    WeightedEnsemble_L3       0.815531   0.813275     roc_auc       17.200876      32.106009  1279.149860                 0.006680                0.070278          13.975391            3       True         12\n",
      "4   LightGBMLarge_BAG_L2       0.815423   0.811520     roc_auc       13.351484      26.031315   958.264097                 1.184200                2.655293         166.295433            2       True         11\n",
      "5         XGBoost_BAG_L2       0.815145   0.812506     roc_auc       14.190454      25.617895   876.288307                 2.023169                2.241872          84.319643            2       True         10\n",
      "6    WeightedEnsemble_L2       0.814743   0.812635     roc_auc        9.665598      18.816411   542.286340                 0.006150                0.069160           9.031925            2       True          6\n",
      "7      LightGBMXT_BAG_L1       0.814005   0.810579     roc_auc        4.027751      10.652899   221.007196                 4.027751               10.652899         221.007196            1       True          1\n",
      "8   LightGBMLarge_BAG_L1       0.813932   0.810369     roc_auc        3.183754       6.131068   226.954431                 3.183754                6.131068         226.954431            1       True          5\n",
      "9         XGBoost_BAG_L1       0.813930   0.810831     roc_auc        2.447942       1.963284    85.292788                 2.447942                1.963284          85.292788            1       True          4\n",
      "10       LightGBM_BAG_L1       0.813198   0.809364     roc_auc        1.786106       4.389443   146.063766                 1.786106                4.389443         146.063766            1       True          2\n",
      "11       CatBoost_BAG_L1       0.810184   0.807446     roc_auc        0.721731       0.239329   112.650483                 0.721731                0.239329         112.650483            1       True          3\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t1467s\t = DyStack   runtime |\t38133s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 38133s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20241104_104436\"\n",
      "Train Data Rows:    413194\n",
      "Train Data Columns: 59\n",
      "Label Column:       target\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    29791.24 MB\n",
      "\tTrain Data (Original)  Memory Usage: 185.99 MB (0.6% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 58 | ['feature_168', 'feature_87', 'feature_72', 'feature_124', 'feature_141', ...]\n",
      "\t\t('int', [])   :  1 | ['id']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 58 | ['feature_168', 'feature_87', 'feature_72', 'feature_124', 'feature_141', ...]\n",
      "\t\t('int', [])   :  1 | ['id']\n",
      "\t3.0s = Fit runtime\n",
      "\t59 features in original data used to generate 59 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 185.99 MB (0.6% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 3.37s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {'auto_class_weights': 'Balanced'},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'XGB': {},\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 25413.31s of the 38129.5s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=4.13%)\n",
      "\t0.8116\t = Validation score   (roc_auc)\n",
      "\t242.76s\t = Training   runtime\n",
      "\t12.47s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 25163.02s of the 37879.2s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=4.14%)\n",
      "\t0.8105\t = Validation score   (roc_auc)\n",
      "\t156.01s\t = Training   runtime\n",
      "\t4.81s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 25002.6s of the 37718.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=4.17%)\n",
      "\t0.8106\t = Validation score   (roc_auc)\n",
      "\t115.36s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 24883.13s of the 37599.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=5.44%)\n",
      "\t0.8111\t = Validation score   (roc_auc)\n",
      "\t92.45s\t = Training   runtime\n",
      "\t2.19s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 24786.51s of the 37502.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=4.25%)\n",
      "\t0.8104\t = Validation score   (roc_auc)\n",
      "\t225.51s\t = Training   runtime\n",
      "\t6.27s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 2541.33s of the 37272.51s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.36, 'XGBoost_BAG_L1': 0.28, 'LightGBMLarge_BAG_L1': 0.24, 'CatBoost_BAG_L1': 0.08, 'LightGBM_BAG_L1': 0.04}\n",
      "\t0.8135\t = Validation score   (roc_auc)\n",
      "\t8.92s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting 5 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 37263.46s of the 37263.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=4.34%)\n",
      "\t0.8137\t = Validation score   (roc_auc)\n",
      "\t126.83s\t = Training   runtime\n",
      "\t3.03s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 37132.26s of the 37132.16s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=4.34%)\n",
      "\t0.8134\t = Validation score   (roc_auc)\n",
      "\t115.72s\t = Training   runtime\n",
      "\t1.7s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 37012.33s of the 37012.23s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=4.38%)\n",
      "\t0.8137\t = Validation score   (roc_auc)\n",
      "\t92.34s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 36916.02s of the 36915.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=5.71%)\n",
      "\t0.8133\t = Validation score   (roc_auc)\n",
      "\t93.59s\t = Training   runtime\n",
      "\t2.54s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 36818.15s of the 36818.04s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=4.47%)\n",
      "\t0.8126\t = Validation score   (roc_auc)\n",
      "\t181.59s\t = Training   runtime\n",
      "\t3.25s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 3726.35s of the 36631.84s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.429, 'XGBoost_BAG_L2': 0.238, 'LightGBM_BAG_L2': 0.143, 'LightGBMLarge_BAG_L2': 0.095, 'LightGBMXT_BAG_L1': 0.048, 'CatBoost_BAG_L2': 0.048}\n",
      "\t0.8141\t = Validation score   (roc_auc)\n",
      "\t15.97s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1517.45s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1399.2 rows/s (51650 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241104_104436\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x798a03c03eb0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor = TabularPredictor(label='target', eval_metric='roc_auc', problem_type='binary')\n",
    "\n",
    "predictor.fit(\n",
    "    train_tab, \n",
    "    time_limit=3600*11,\n",
    "    hyperparameters={\n",
    "        'CAT': {'auto_class_weights': 'Balanced'},\n",
    "        'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
    "        'XGB': {}\n",
    "    },\n",
    "    ag_args_fit={'num_gpus': 1},\n",
    "    num_gpus=1,\n",
    "    presets='best_quality'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da479140",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T11:34:21.895352Z",
     "iopub.status.busy": "2024-11-04T11:34:21.894373Z",
     "iopub.status.idle": "2024-11-04T11:35:22.761045Z",
     "shell.execute_reply": "2024-11-04T11:35:22.760174Z"
    },
    "papermill": {
     "duration": 60.91728,
     "end_time": "2024-11-04T11:35:22.763693",
     "exception": false,
     "start_time": "2024-11-04T11:34:21.846413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = predictor.predict_proba(test_tab)[1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4226d728",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T11:35:22.865040Z",
     "iopub.status.busy": "2024-11-04T11:35:22.864591Z",
     "iopub.status.idle": "2024-11-04T11:35:23.761200Z",
     "shell.execute_reply": "2024-11-04T11:35:23.760107Z"
    },
    "papermill": {
     "duration": 0.951662,
     "end_time": "2024-11-04T11:35:23.763628",
     "exception": false,
     "start_time": "2024-11-04T11:35:22.811966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        'id': id_value,\n",
    "        'target': pred_value   \n",
    "    } \n",
    "    for id_value, pred_value in zip(ids, pred)\n",
    "]\n",
    "\n",
    "csv_filename = 'submission_autogluon.csv'\n",
    "\n",
    "with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "    fieldnames = ['id', 'target']\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in data:\n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5397822,
     "sourceId": 8966877,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5398217,
     "sourceId": 8967399,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5993029,
     "sourceId": 9782155,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3321.12668,
   "end_time": "2024-11-04T11:35:28.933097",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-04T10:40:07.806417",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
