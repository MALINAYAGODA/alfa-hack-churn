{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b485be",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-04T10:40:10.541705Z",
     "iopub.status.busy": "2024-11-04T10:40:10.540903Z",
     "iopub.status.idle": "2024-11-04T10:40:13.630655Z",
     "shell.execute_reply": "2024-11-04T10:40:13.629679Z"
    },
    "papermill": {
     "duration": 3.098486,
     "end_time": "2024-11-04T10:40:13.633019",
     "exception": false,
     "start_time": "2024-11-04T10:40:10.534533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from catboost.utils import get_gpu_device_count\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81c31514",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T10:40:13.643830Z",
     "iopub.status.busy": "2024-11-04T10:40:13.643363Z",
     "iopub.status.idle": "2024-11-04T10:40:13.714032Z",
     "shell.execute_reply": "2024-11-04T10:40:13.713150Z"
    },
    "papermill": {
     "duration": 0.077985,
     "end_time": "2024-11-04T10:40:13.715944",
     "exception": false,
     "start_time": "2024-11-04T10:40:13.637959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPU'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_gpu_available = get_gpu_device_count()\n",
    "device = 'GPU' if is_gpu_available else 'CPU'\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4647d89c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T10:40:13.726785Z",
     "iopub.status.busy": "2024-11-04T10:40:13.726097Z",
     "iopub.status.idle": "2024-11-04T10:40:13.730280Z",
     "shell.execute_reply": "2024-11-04T10:40:13.729469Z"
    },
    "papermill": {
     "duration": 0.011601,
     "end_time": "2024-11-04T10:40:13.732227",
     "exception": false,
     "start_time": "2024-11-04T10:40:13.720626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_PATH = r'/data/Кейс-3. Отток юридических лиц из расчетно-кассового обслуживания/train'\n",
    "TEST_PATH = r'/data/Кейс-3. Отток юридических лиц из расчетно-кассового обслуживания/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90faae2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T10:40:13.742319Z",
     "iopub.status.busy": "2024-11-04T10:40:13.742056Z",
     "iopub.status.idle": "2024-11-04T10:40:45.916488Z",
     "shell.execute_reply": "2024-11-04T10:40:45.915420Z"
    },
    "papermill": {
     "duration": 32.1821,
     "end_time": "2024-11-04T10:40:45.918885",
     "exception": false,
     "start_time": "2024-11-04T10:40:13.736785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    filenames_train = glob.glob(path + '/*.csv')\n",
    "    data_files_train = []\n",
    "    \n",
    "    for filename in filenames_train:\n",
    "        data_files_train.append(pd.read_csv(filename))\n",
    "\n",
    "    return pd.concat(data_files_train, ignore_index=True)\n",
    "\n",
    "train_df = get_data(TRAIN_PATH)\n",
    "test_df = get_data(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15d250c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T10:40:45.929503Z",
     "iopub.status.busy": "2024-11-04T10:40:45.929185Z",
     "iopub.status.idle": "2024-11-04T10:40:45.943878Z",
     "shell.execute_reply": "2024-11-04T10:40:45.942939Z"
    },
    "papermill": {
     "duration": 0.022061,
     "end_time": "2024-11-04T10:40:45.945820",
     "exception": false,
     "start_time": "2024-11-04T10:40:45.923759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ids = test_df['id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe7c722",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T10:40:45.957672Z",
     "iopub.status.busy": "2024-11-04T10:40:45.956965Z",
     "iopub.status.idle": "2024-11-04T10:40:45.962996Z",
     "shell.execute_reply": "2024-11-04T10:40:45.962212Z"
    },
    "papermill": {
     "duration": 0.013275,
     "end_time": "2024-11-04T10:40:45.964897",
     "exception": false,
     "start_time": "2024-11-04T10:40:45.951622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = [\n",
    "       'feature_168', 'feature_87', 'feature_72', 'feature_124', 'feature_141',\n",
    "       'feature_29', 'feature_55', 'feature_142', 'feature_78', 'feature_183',\n",
    "       'feature_84', 'feature_146', 'feature_134', 'feature_26', 'feature_12',\n",
    "       'feature_127', 'feature_59', 'feature_100', 'feature_96', 'feature_112',\n",
    "       'feature_169', 'feature_16', 'feature_76', 'feature_81', 'feature_79',\n",
    "       'feature_22', 'feature_152', 'feature_43', 'feature_20', 'feature_18',\n",
    "       'feature_44', 'id', 'feature_177', 'feature_50', 'feature_6',\n",
    "       'feature_66', 'feature_9', 'feature_46', 'feature_103', 'feature_75',\n",
    "       'feature_8', 'feature_36', 'feature_41', 'feature_184', 'feature_62',\n",
    "       'feature_95', 'feature_133', 'feature_28', 'feature_108', 'feature_128',\n",
    "       'feature_117', 'feature_161', 'feature_157', 'feature_107', 'feature_147'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff26f95b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T10:40:45.975221Z",
     "iopub.status.busy": "2024-11-04T10:40:45.974734Z",
     "iopub.status.idle": "2024-11-04T10:40:46.063234Z",
     "shell.execute_reply": "2024-11-04T10:40:46.062426Z"
    },
    "papermill": {
     "duration": 0.096306,
     "end_time": "2024-11-04T10:40:46.065646",
     "exception": false,
     "start_time": "2024-11-04T10:40:45.969340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = train_df[f + ['target']]\n",
    "test_df = test_df[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fadb9bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T10:40:46.076802Z",
     "iopub.status.busy": "2024-11-04T10:40:46.076173Z",
     "iopub.status.idle": "2024-11-04T10:40:46.111683Z",
     "shell.execute_reply": "2024-11-04T10:40:46.110651Z"
    },
    "papermill": {
     "duration": 0.043507,
     "end_time": "2024-11-04T10:40:46.114020",
     "exception": false,
     "start_time": "2024-11-04T10:40:46.070513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "feature_operations = {\n",
    "    'atan_feature_112': ('feature_112', np.arctan),\n",
    "    'log_feature_26': ('feature_26', np.log1p),\n",
    "    'sin_feature_26': ('feature_26', np.sin),\n",
    "    'tan_feature_26': ('feature_26', np.tan)\n",
    "}\n",
    "\n",
    "for new_feature, (base_feature, operation) in feature_operations.items():\n",
    "    train_df[new_feature] = operation(train_df[base_feature])\n",
    "    test_df[new_feature] = operation(test_df[base_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7bef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"autogluon==1.1.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6a5d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"scikit_learn==1.2.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1c3c52f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T10:44:35.812659Z",
     "iopub.status.busy": "2024-11-04T10:44:35.811852Z",
     "iopub.status.idle": "2024-11-04T10:44:36.740849Z",
     "shell.execute_reply": "2024-11-04T10:44:36.739865Z"
    },
    "papermill": {
     "duration": 0.958163,
     "end_time": "2024-11-04T10:44:36.743188",
     "exception": false,
     "start_time": "2024-11-04T10:44:35.785025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1d8339d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T10:44:36.796575Z",
     "iopub.status.busy": "2024-11-04T10:44:36.795793Z",
     "iopub.status.idle": "2024-11-04T10:44:36.800570Z",
     "shell.execute_reply": "2024-11-04T10:44:36.799715Z"
    },
    "papermill": {
     "duration": 0.033178,
     "end_time": "2024-11-04T10:44:36.802381",
     "exception": false,
     "start_time": "2024-11-04T10:44:36.769203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_tab = TabularDataset(train_df)\n",
    "test_tab = TabularDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec69b44d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T10:44:36.856144Z",
     "iopub.status.busy": "2024-11-04T10:44:36.855341Z",
     "iopub.status.idle": "2024-11-04T11:34:21.797124Z",
     "shell.execute_reply": "2024-11-04T11:34:21.796312Z"
    },
    "papermill": {
     "duration": 2984.970014,
     "end_time": "2024-11-04T11:34:21.799129",
     "exception": false,
     "start_time": "2024-11-04T10:44:36.829115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20241104_104436\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.14\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       29.83 GB / 31.36 GB (95.1%)\n",
      "Disk Space Avail:   19.50 GB / 19.52 GB (99.9%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 9900s of the 39600s of remaining time (25%).\n",
      "2024-11-04 10:44:37,172\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2024-11-04 10:44:39,735\tINFO worker.py:1743 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"AutogluonModels/ag-20241104_104436/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Beginning AutoGluon training ... Time limit = 9896s\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m AutoGluon will save models to \"AutogluonModels/ag-20241104_104436/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Train Data Rows:    367283\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Train Data Columns: 59\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Label Column:       target\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Problem Type:       binary\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tAvailable Memory:                    29683.98 MB\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tTrain Data (Original)  Memory Usage: 165.33 MB (0.6% of available memory)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\t('float', []) : 58 | ['feature_168', 'feature_87', 'feature_72', 'feature_124', 'feature_141', ...]\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\t('int', [])   :  1 | ['id']\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\t('float', []) : 58 | ['feature_168', 'feature_87', 'feature_72', 'feature_124', 'feature_141', ...]\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\t('int', [])   :  1 | ['id']\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t3.6s = Fit runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t59 features in original data used to generate 59 features in processed data.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tTrain Data (Processed) Memory Usage: 165.33 MB (0.6% of available memory)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Data preprocessing and feature engineering runtime = 3.98s ...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t'CAT': {'auto_class_weights': 'Balanced'},\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t'XGB': {},\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting 5 L1 models ...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 6592.86s of the 9891.76s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=3.76%)\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=491)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=526)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=561)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=596)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=631)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=666)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=701)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=736)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.8106\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t221.01s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t10.65s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 6363.89s of the 9662.79s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=3.79%)\n",
      "\u001b[36m(_ray_fit pid=849)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=884)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=919)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=954)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=989)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1024)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1059)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1094)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.8094\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t146.06s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t4.39s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 6212.76s of the 9511.66s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=3.83%)\n",
      "\u001b[36m(_ray_fit pid=1207)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1207)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=1251)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1251)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=1295)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1295)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=1339)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1339)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=1383)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1383)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=1427)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1427)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=1471)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1471)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=1515)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1515)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.8074\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t112.65s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.24s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 6095.64s of the 9394.54s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=4.98%)\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:16] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:16] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:18] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:18] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=1637)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:27] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:27] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:29] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:29] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=1668)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=1699)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:38] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=1699)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1699)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1699)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1699)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1699)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:40] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1699)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1699)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:40] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=1699)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=1699)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=1699)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=1699)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=1730)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:49] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=1730)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1730)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1730)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:49] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1730)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1730)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:51] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1730)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1730)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:53:51] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=1730)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=1730)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=1730)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=1730)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:00] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:00] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:01] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:01] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=1761)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=1792)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:11] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=1792)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1792)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1792)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:11] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1792)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1792)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:13] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1792)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1792)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:13] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=1792)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=1792)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=1792)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=1792)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=1823)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:22] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=1823)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1823)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1823)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:22] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1823)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1823)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:24] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1823)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1823)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:24] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=1823)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=1823)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=1823)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=1823)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:33] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:33] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:34] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:54:34] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.8108\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t85.29s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t1.96s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 6005.63s of the 9304.53s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=3.90%)\n",
      "\u001b[36m(_ray_fit pid=1963)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1854)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1998)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2033)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2068)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2103)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2138)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2173)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2208)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.8104\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t226.95s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t6.13s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 659.29s of the 9072.1s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.353, 'XGBoost_BAG_L1': 0.353, 'LightGBMLarge_BAG_L1': 0.294}\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.8126\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t9.03s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.07s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting 5 L2 models ...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 9062.96s of the 9062.86s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=3.97%)\n",
      "\u001b[36m(_ray_fit pid=2327)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2362)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2397)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2432)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2467)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2502)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2537)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2572)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.8128\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t114.88s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t2.36s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 8943.67s of the 8943.58s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=3.97%)\n",
      "\u001b[36m(_ray_fit pid=2691)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2726)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2761)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2796)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2831)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2866)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2901)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2936)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.8124\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t107.71s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t1.4s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 8831.71s of the 8831.62s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=4.01%)\n",
      "\u001b[36m(_ray_fit pid=3055)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3055)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=3099)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3099)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=3143)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3143)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=3187)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3187)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=3231)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3231)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=3275)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3275)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=3319)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3319)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=3363)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3363)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.8127\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t100.02s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.25s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: XGBoost_BAG_L2 ... Training model for up to 8727.49s of the 8727.4s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=5.22%)\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:24] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:24] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:25] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:25] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=3491)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=3522)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:34] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=3522)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3522)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:34] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3522)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3522)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3522)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:35] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3522)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3522)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:35] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=3522)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=3522)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=3522)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=3522)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=3553)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:45] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=3553)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3553)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3553)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:45] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3553)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3553)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:46] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3553)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3553)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:46] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=3553)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=3553)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=3553)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=3553)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=3584)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:56] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=3584)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3584)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3584)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:56] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3584)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3584)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:57] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3584)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3584)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:04:57] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=3584)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=3584)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=3584)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=3584)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=3615)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:07] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=3615)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3615)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3615)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:07] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3615)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3615)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:08] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3615)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3615)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:08] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=3615)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=3615)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=3615)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=3615)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=3646)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:18] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=3646)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3646)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3646)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:18] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3646)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3646)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:19] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3646)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3646)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:19] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=3646)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=3646)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=3646)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=3646)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=3677)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:28] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=3677)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3677)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3677)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3677)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3677)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:30] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3677)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3677)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:30] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=3677)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=3677)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=3677)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=3677)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:39] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:39] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:40] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [11:05:40] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.8125\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t84.32s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t2.24s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 8638.61s of the 8638.52s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=4.10%)\n",
      "\u001b[36m(_ray_fit pid=3823)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3708)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3858)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3893)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3928)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3963)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3998)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=4033)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=4068)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.8115\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t166.3s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t2.66s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 906.29s of the 8467.66s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.35, 'XGBoost_BAG_L2': 0.3, 'XGBoost_BAG_L1': 0.1, 'LightGBMLarge_BAG_L1': 0.1, 'LightGBMXT_BAG_L1': 0.05, 'LightGBM_BAG_L2': 0.05, 'LightGBMLarge_BAG_L2': 0.05}\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.8133\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t13.98s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.07s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m AutoGluon training complete, total runtime = 1442.46s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1432.7 rows/s (45911 batch size)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241104_104436/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                   model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0        CatBoost_BAG_L2       0.815692   0.812735     roc_auc       12.462878      23.630606   891.990016                 0.295594                0.254583         100.021352            2       True          9\n",
      "1        LightGBM_BAG_L2       0.815579   0.812390     roc_auc       12.926130      24.778124   899.680461                 0.758845                1.402102         107.711797            2       True          8\n",
      "2      LightGBMXT_BAG_L2       0.815572   0.812794     roc_auc       13.227981      25.736464   906.847595                 1.060697                2.360441         114.878931            2       True          7\n",
      "3    WeightedEnsemble_L3       0.815531   0.813275     roc_auc       17.200876      32.106009  1279.149860                 0.006680                0.070278          13.975391            3       True         12\n",
      "4   LightGBMLarge_BAG_L2       0.815423   0.811520     roc_auc       13.351484      26.031315   958.264097                 1.184200                2.655293         166.295433            2       True         11\n",
      "5         XGBoost_BAG_L2       0.815145   0.812506     roc_auc       14.190454      25.617895   876.288307                 2.023169                2.241872          84.319643            2       True         10\n",
      "6    WeightedEnsemble_L2       0.814743   0.812635     roc_auc        9.665598      18.816411   542.286340                 0.006150                0.069160           9.031925            2       True          6\n",
      "7      LightGBMXT_BAG_L1       0.814005   0.810579     roc_auc        4.027751      10.652899   221.007196                 4.027751               10.652899         221.007196            1       True          1\n",
      "8   LightGBMLarge_BAG_L1       0.813932   0.810369     roc_auc        3.183754       6.131068   226.954431                 3.183754                6.131068         226.954431            1       True          5\n",
      "9         XGBoost_BAG_L1       0.813930   0.810831     roc_auc        2.447942       1.963284    85.292788                 2.447942                1.963284          85.292788            1       True          4\n",
      "10       LightGBM_BAG_L1       0.813198   0.809364     roc_auc        1.786106       4.389443   146.063766                 1.786106                4.389443         146.063766            1       True          2\n",
      "11       CatBoost_BAG_L1       0.810184   0.807446     roc_auc        0.721731       0.239329   112.650483                 0.721731                0.239329         112.650483            1       True          3\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t1467s\t = DyStack   runtime |\t38133s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 38133s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20241104_104436\"\n",
      "Train Data Rows:    413194\n",
      "Train Data Columns: 59\n",
      "Label Column:       target\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    29791.24 MB\n",
      "\tTrain Data (Original)  Memory Usage: 185.99 MB (0.6% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 58 | ['feature_168', 'feature_87', 'feature_72', 'feature_124', 'feature_141', ...]\n",
      "\t\t('int', [])   :  1 | ['id']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 58 | ['feature_168', 'feature_87', 'feature_72', 'feature_124', 'feature_141', ...]\n",
      "\t\t('int', [])   :  1 | ['id']\n",
      "\t3.0s = Fit runtime\n",
      "\t59 features in original data used to generate 59 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 185.99 MB (0.6% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 3.37s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {'auto_class_weights': 'Balanced'},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'XGB': {},\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 25413.31s of the 38129.5s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=4.13%)\n",
      "\t0.8116\t = Validation score   (roc_auc)\n",
      "\t242.76s\t = Training   runtime\n",
      "\t12.47s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 25163.02s of the 37879.2s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=4.14%)\n",
      "\t0.8105\t = Validation score   (roc_auc)\n",
      "\t156.01s\t = Training   runtime\n",
      "\t4.81s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 25002.6s of the 37718.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=4.17%)\n",
      "\t0.8106\t = Validation score   (roc_auc)\n",
      "\t115.36s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 24883.13s of the 37599.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=5.44%)\n",
      "\t0.8111\t = Validation score   (roc_auc)\n",
      "\t92.45s\t = Training   runtime\n",
      "\t2.19s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 24786.51s of the 37502.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=4.25%)\n",
      "\t0.8104\t = Validation score   (roc_auc)\n",
      "\t225.51s\t = Training   runtime\n",
      "\t6.27s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 2541.33s of the 37272.51s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.36, 'XGBoost_BAG_L1': 0.28, 'LightGBMLarge_BAG_L1': 0.24, 'CatBoost_BAG_L1': 0.08, 'LightGBM_BAG_L1': 0.04}\n",
      "\t0.8135\t = Validation score   (roc_auc)\n",
      "\t8.92s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting 5 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 37263.46s of the 37263.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=4.34%)\n",
      "\t0.8137\t = Validation score   (roc_auc)\n",
      "\t126.83s\t = Training   runtime\n",
      "\t3.03s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 37132.26s of the 37132.16s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=4.34%)\n",
      "\t0.8134\t = Validation score   (roc_auc)\n",
      "\t115.72s\t = Training   runtime\n",
      "\t1.7s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 37012.33s of the 37012.23s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=4.38%)\n",
      "\t0.8137\t = Validation score   (roc_auc)\n",
      "\t92.34s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 36916.02s of the 36915.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=5.71%)\n",
      "\t0.8133\t = Validation score   (roc_auc)\n",
      "\t93.59s\t = Training   runtime\n",
      "\t2.54s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 36818.15s of the 36818.04s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=4.47%)\n",
      "\t0.8126\t = Validation score   (roc_auc)\n",
      "\t181.59s\t = Training   runtime\n",
      "\t3.25s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 3726.35s of the 36631.84s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.429, 'XGBoost_BAG_L2': 0.238, 'LightGBM_BAG_L2': 0.143, 'LightGBMLarge_BAG_L2': 0.095, 'LightGBMXT_BAG_L1': 0.048, 'CatBoost_BAG_L2': 0.048}\n",
      "\t0.8141\t = Validation score   (roc_auc)\n",
      "\t15.97s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1517.45s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1399.2 rows/s (51650 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241104_104436\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x798a03c03eb0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor = TabularPredictor(label='target', eval_metric='roc_auc', problem_type='binary')\n",
    "\n",
    "predictor.fit(\n",
    "    train_tab, \n",
    "    time_limit=3600*11,\n",
    "    hyperparameters={\n",
    "        'CAT': {'auto_class_weights': 'Balanced'},\n",
    "        'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
    "        'XGB': {}\n",
    "    },\n",
    "    ag_args_fit={'num_gpus': 1},\n",
    "    num_gpus=1,\n",
    "    presets='best_quality'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da479140",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T11:34:21.895352Z",
     "iopub.status.busy": "2024-11-04T11:34:21.894373Z",
     "iopub.status.idle": "2024-11-04T11:35:22.761045Z",
     "shell.execute_reply": "2024-11-04T11:35:22.760174Z"
    },
    "papermill": {
     "duration": 60.91728,
     "end_time": "2024-11-04T11:35:22.763693",
     "exception": false,
     "start_time": "2024-11-04T11:34:21.846413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = predictor.predict_proba(test_tab)[1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4226d728",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T11:35:22.865040Z",
     "iopub.status.busy": "2024-11-04T11:35:22.864591Z",
     "iopub.status.idle": "2024-11-04T11:35:23.761200Z",
     "shell.execute_reply": "2024-11-04T11:35:23.760107Z"
    },
    "papermill": {
     "duration": 0.951662,
     "end_time": "2024-11-04T11:35:23.763628",
     "exception": false,
     "start_time": "2024-11-04T11:35:22.811966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        'id': id_value,\n",
    "        'target': pred_value   \n",
    "    } \n",
    "    for id_value, pred_value in zip(ids, pred)\n",
    "]\n",
    "\n",
    "csv_filename = 'submission_autogluon.csv'\n",
    "\n",
    "with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "    fieldnames = ['id', 'target']\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in data:\n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5397822,
     "sourceId": 8966877,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5398217,
     "sourceId": 8967399,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5993029,
     "sourceId": 9782155,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3321.12668,
   "end_time": "2024-11-04T11:35:28.933097",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-04T10:40:07.806417",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
